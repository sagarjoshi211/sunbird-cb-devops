pipeline {
    agent any

    environment {
        local_backup_dir = "/db_backup/elasticsearch"
        nfs_mount_point = "/elasticsearch_backup"
        retention_days = 3
        source_data_dir = "/tmp"
        // nfs_server = "nfs_server:/nfs/share"
    }

    stages {
        stage('Backup and Transfer Data') {
            steps {
                script {
                    // Create local backup directory
                    sh "mkdir -p ${local_backup_dir}"

                    // Find and copy old files to local backup directory
                    def oldFiles = sh(script: "sudo find ${source_data_dir} -type f -mtime +${retention_days}", returnStdout: true).trim().split('\n')
                    if (oldFiles) {
                        sh "sudo rsync ${oldFiles.join(' ')} ${local_backup_dir}/"
                    }
                }
            }
        }

        stage('Transfer to NFS') {
            steps {
                script {
                    // List files in local backup directory
                    def filesToCopy = sh(script: "ls -A ${local_backup_dir}", returnStdout: true).trim().split('\n')
                    
                    if (filesToCopy) {
                        // Create NFS backup directory
                        sh "sudo mkdir -p ${nfs_mount_point}/backup"

                        // Copy files to NFS backup directory
                        sh "sudo cp -r ${local_backup_dir}/* ${nfs_mount_point}/backup/"
                    }
                }
            }
        }
    }

    post {
        always {
            echo 'Cleaning up...'

            // Optionally, you can clean up local backup files
            sh "sudo rm -rf ${local_backup_dir}"
        }

        success {
            echo 'Elasticsearch backup and transfer were successful.'
        }

        failure {
            echo 'Elasticsearch backup or transfer failed.'
            // You can add additional steps to handle failure scenarios
        }
    }
}
